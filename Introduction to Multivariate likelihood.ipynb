{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would like to start first with the simple case of a multivariate estimation of two variables observed jointly.\n",
    "\n",
    "\n",
    "$$ x_1 = N(1,1)  $$\n",
    "\n",
    "\n",
    "$$ x_2 = 0.25*x_1 + N(0,0.5) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a random normal\n",
    "np.random.seed = 20\n",
    "x1 = 1 + np.random.normal(0,1,100)\n",
    "#creating another random normal that is correlated with x1\n",
    "x2 = 0.25*x1 + np.random.normal(0,0.5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In handleing multivariates it's important to know how to handle matrix multiplication correctly.\n",
    "\n",
    "The quadratic term appears inside the maximum likelihood estimator is formed through multiplying a row vector times a matrix and then multiplying again by the transpose of the vector. \n",
    "\n",
    "\n",
    "The vector (row vector) that contians a joint observation of the multivariate normal looks like this:\n",
    "\n",
    "$$ X_i = [ x_{1,i} , x_{2,i}] $$\n",
    "\n",
    "\n",
    "$$ X_i\\Sigma X'_i $$\n",
    "\n",
    "\n",
    "And for N observations it's desirable to compute the following summation:\n",
    "\n",
    "\n",
    "$$ \\sum{X_i\\Sigma X'_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343.54755902032286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It can be done through the loop:\n",
    "X = np.column_stack((x1,x2))\n",
    "L = np.zeros(100)\n",
    "for i in range (len(X)):\n",
    "    h_i =   np.matmul(np.matmul(X[i,],np.cov(x1,x2)),X[i,].reshape(-1,1))\n",
    "    L[i] = h_i.item()\n",
    "\n",
    "np.sum(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next step is to write the Bivariate likelihood:\n",
    "\n",
    "\n",
    "$$ \\ell(\\mu,\\Sigma) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(|\\Sigma|)  - \\frac{1}{2} \\sum{X_i\\Sigma X'_i}  $$\n",
    "\n",
    "\n",
    "$$\\hat \\mu = [\\hat \\mu_{1} , \\hat \\mu_{2}]$$ \n",
    "\n",
    "\n",
    "$$ \\hat \\Sigma = \\begin{bmatrix} \n",
    "\\hat \\sigma_{1}^2  & \\hat \\sigma_{12} \\\\\n",
    "\\hat \\sigma_{12} & \\hat \\sigma_{2}^2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "And the likelihood is maximized with respect to the vector of unknown parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The likelihood function:\n",
    "def log_likelihood(p,x1,x2):\n",
    "    mu1_hat = p[0]\n",
    "    mu2_hat = p[1]\n",
    "    sigma1_hat = p[2]\n",
    "    sigma2_hat = p[3]\n",
    "    sigma12_hat = p[4]\n",
    "    mu = np.column_stack((mu1_hat,mu2_hat))\n",
    "    X = np.column_stack((x1,x2))\n",
    "    V = np.array([[sigma1_hat,sigma12_hat], [sigma12_hat,sigma2_hat]])\n",
    "    Det_V = np.linalg.det(V)\n",
    "    V_inv = np.linalg.inv(V)\n",
    "    L = np.zeros(len(X))\n",
    "    for i in range (len(X)):\n",
    "      h_i =   np.matmul(np.matmul(X[i,] - mu,V_inv),(X[i,]-mu).reshape(-1,1))\n",
    "      L[i] = h_i.item()\n",
    "\n",
    "    l = -0.5*(len(X))*np.log(2*np.pi) - 0.5*(len(X))*np.log(Det_V) - 0.5*np.sum(L)\n",
    "    return -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.51613285457668"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0 = [0.9,0.4,0.7,0.5,0]\n",
    "ll = log_likelihood(p0,x1,x2)\n",
    "ll #This is the likelihood of some arbitrary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the optimization on the above function, one would like to mahe sure the diagonal terms on the matrix $ \\hat \\Sigma $ remains positive throughout the entire optimization iterations.\n",
    "\n",
    "\n",
    "We want : $$ \\hat \\sigma_{1}^2 > 0 $$ and $$\\hat \\sigma_{2}^2 > 0 $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint(p):\n",
    "    sigma1_hat = p[2]\n",
    "    sigma2_hat = p[3]\n",
    "    return sigma1_hat ,sigma2_hat\n",
    "\n",
    "cons = { 'type' :'ineq' , 'fun' : constraint}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minoa\\AppData\\Local\\Temp\\ipykernel_19828\\2844859708.py:18: RuntimeWarning: invalid value encountered in log\n",
      "  l = -0.5*(len(X))*np.log(2*np.pi) - 0.5*(len(X))*np.log(Det_V) - 0.5*np.sum(L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " message: Optimization terminated successfully\n",
       " success: True\n",
       "  status: 0\n",
       "     fun: 116.88785395465577\n",
       "       x: [ 1.005e+00  3.007e-01  7.984e-01  3.231e-01  1.868e-01]\n",
       "     nit: 14\n",
       "     jac: [ 7.677e-04 -2.972e-03  2.509e-03 -7.340e-03  7.915e-04]\n",
       "    nfev: 96\n",
       "    njev: 14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(log_likelihood, p0, args= (x1,x2),constraints= cons, options={\"maxiter\": 1000} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of explicitly passing the restrictions to the solver one can use the Cholesky decomposition on the $\\Sigma$ matrix\n",
    "\n",
    "\n",
    "$$ P = \\begin{bmatrix} \n",
    "\\lambda_{1}  &  0 \\\\\n",
    "\\lambda_{12} &  \\lambda_{2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$ P' = \\begin{bmatrix} \n",
    "\\lambda_{1}  &  \\lambda_{12} \\\\\n",
    "0 &  \\lambda_{2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\\Sigma = \\begin{bmatrix} \n",
    "\\lambda_{1}^2  &  \\lambda_{1} \\lambda_{12} \\\\\n",
    "\\lambda_{1} \\lambda_{12} &  \\lambda_{12}^2+\\lambda_{2}^2 \n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The likelihood function:\n",
    "def log_likelihood(p,x1,x2):\n",
    "    mu1_hat = p[0]\n",
    "    mu2_hat = p[1]\n",
    "    lam1_hat = p[2]\n",
    "    lam2_hat = p[3]\n",
    "    lam12_hat = p[4]\n",
    "    mu = np.column_stack((mu1_hat,mu2_hat))\n",
    "    X = np.column_stack((x1,x2))\n",
    "    P  = np.array([[lam1_hat,0], [lam12_hat,lam2_hat]])\n",
    "    P_T = np.array([[lam1_hat,lam12_hat], [0,lam2_hat]])\n",
    "    V = np.matmul(P,P_T)\n",
    "    Det_V = np.linalg.det(V)\n",
    "    V_inv = np.linalg.inv(V)\n",
    "    L = np.zeros(len(X))\n",
    "    for i in range (len(X)):\n",
    "      h_i =   np.matmul(np.matmul(X[i,] - mu,V_inv),(X[i,]-mu).reshape(-1,1))\n",
    "      L[i] = h_i.item()\n",
    "\n",
    "    l = -0.5*(len(X))*np.log(2*np.pi) - 0.5*(len(X))*np.log(Det_V) - 0.5*np.sum(L)\n",
    "    return -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370.42127300154607"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the likelihood function with any arbitrary guess of the parameters:\n",
    "p0 = [0,0.9, 1,4,2]\n",
    "ll = log_likelihood(p0,x1,x2)\n",
    "ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 115.9333899716299\n",
       "        x: [ 1.132e+00  2.714e-01  8.904e-01 -5.254e-01  7.401e-02]\n",
       "      nit: 21\n",
       "      jac: [ 9.537e-07  9.537e-07  0.000e+00 -9.537e-07  1.907e-06]\n",
       " hess_inv: [[ 8.123e-03  5.223e-04 ...  2.473e-04  8.362e-06]\n",
       "            [ 5.223e-04  2.800e-03 ...  3.210e-06 -3.856e-06]\n",
       "            ...\n",
       "            [ 2.473e-04  3.210e-06 ...  1.464e-03 -6.347e-06]\n",
       "            [ 8.362e-06 -3.856e-06 ... -6.347e-06  2.735e-03]]\n",
       "     nfev: 210\n",
       "     njev: 35"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(log_likelihood, p0, args= (x1,x2), options={\"maxiter\": 1000} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the estimates for the mean are immediately reported on the first two elements of the output of the solver.\n",
    "\n",
    "\n",
    "$$\\hat \\mu = [ 1.13 , 0.27]$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for the Variance-Covariance Matrix:\n",
    "\n",
    "First retrive the $\\lambda's$\n",
    "\n",
    "$$ P = \\begin{bmatrix} \n",
    "0.89  &  0 \\\\\n",
    "0.074 &  -0.525 \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.654481, 0.059866],\n",
       "       [0.059866, 0.281101]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([[0.809,0], [0.074,-0.525]])\n",
    "V = np.matmul(P,P.T)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat \\Sigma = PP'=  \\begin{bmatrix} \n",
    "0.654  &  0.059 \\\\\n",
    "0.059 &  0.281 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
